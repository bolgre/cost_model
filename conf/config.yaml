experiment:
    name: "1000_xavier"
    base_path: "/prj/qct/coredev/hexagon/austin/teams/llvm/Halide/autosched/eunjpark/cost_model_halide"

data_generation:
    train_dataset_file: "/prj/qct/coredev/hexagon/austin/teams/llvm/Halide/autosched/eunjpark/cost_model_halide/1000_train.json"
    valid_dataset_file: "/prj/qct/coredev/hexagon/austin/teams/llvm/Halide/autosched/eunjpark/cost_model_halide/stencil_chain.json"
    benchmark_dataset_file: "/prj/qct/coredev/hexagon/austin/teams/llvm/Halide/autosched/eunjpark/cost_model_halide/stencil_chain.json"
    dataset_name:  "1000_batches"
    batch_size: 1024
    nb_processes: 200

training:
    log_file: "logs.txt" # Just the name
    lr: 0.001
    max_epochs: 500
#    training_gpu: "cuda:5"
    training_gpu: "cpu"
    validation_gpu: "cpu"
    continue_training: False
    model_weights_path: "/prj/qct/coredev/hexagon/austin/teams/llvm/Halide/autosched/eunjpark/cost_model_halide/test/train_with_1000_batches.pt"

testing:
    testing_model_weights_path: "/prj/qct/coredev/hexagon/austin/teams/llvm/Halide/autosched/eunjpark/cost_model_halide/test/test_with_1000_batches.pt" # Model weights to evaluate
    pred_file: "/prj/qct/coredev/hexagon/austin/teams/llvm/Halide/autosched/eunjpark/cost_model_halide/predictions.txt"
    gpu: "cpu" # GPU to validate on

wandb:
    use_wandb: True
    project: "Halide-newdata-xavier"

model:
    input_size: 846
#    input_size: 1790
    comp_embed_layer_sizes:
        - 600
        - 350
        - 200
        - 180
    drops:
        - 0.050
        - 0.050
        - 0.050
        - 0.050
        - 0.050

defaults:
  - override hydra/job_logging: disabled
